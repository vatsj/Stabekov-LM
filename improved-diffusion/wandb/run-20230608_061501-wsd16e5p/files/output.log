creating data loader...
load data **************************************************
bert loading
Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 128, padding_idx=0)
    (position_embeddings): Embedding(512, 128)
    (token_type_embeddings): Embedding(2, 128)
    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-1): 2 x BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=128, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
hello loading text data.
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 128, padding_idx=0)
    (position_embeddings): Embedding(512, 128)
    (token_type_embeddings): Embedding(2, 128)
    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-1): 2 x BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=128, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
Namespace(attention_resolutions='16,8', batch_size=64, cache_mode='no', checkpoint_path='diffusion_models/diff_bert_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e', class_cond=False, commonGen_train='diffusion_lm/common-gen/commongen_data', config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml', config_name='bert-base-uncased', data_dir='', dataset_config_name='wikitext-2-raw-v1', dataset_name='wikitext', diffusion_steps=2000, dropout=0.1, e2e_train='e2e_data', ema_rate='0.9999', emb_scale_factor=1.0, eval_interval=2000, experiment='random', experiment_mode='lm', fp16_scale_growth=0.001, gradient_clipping=-1.0, image_size=8, in_channel=128, learn_sigma=False, log_interval=50, logits_mode=1, lr=0.0001, lr_anneal_steps=400000, microbatch=-1, modality='bert', model_arch='transformer', model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None', noise_level=0.0, noise_schedule='sqrt', num_channels=128, num_heads=4, num_heads_upsample=-1, num_res_blocks=2, out_channel=128, padding_mode='pad', predict_xstart=True, preprocessing_num_workers=1, rescale_learned_sigmas=True, rescale_timesteps=True, resume_checkpoint='', roc_train='../datasets/ROCstory', save_interval=50000, schedule_sampler='uniform', seed=101, sigma_small=False, timestep_respacing='', training_mode='bert', use_bert_tokenizer='no', use_checkpoint=False, use_fp16=False, use_kl=False, use_scale_shift_norm=True, vocab_size=11043, weight_decay=0.0, wiki_train='diffusion_lm/simple_wiki/data.v1.split/simple.training.txt', yelp_train='diffusion_lm/yelpnlg-resources/yelpnlg-corpus')
**task_mode** bert
loading initialized random embeddings.
IN GET CORPUS ROCSTORY POG POG POG Namespace(attention_resolutions='16,8', batch_size=64, cache_mode='no', checkpoint_path='diffusion_models/diff_bert_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e', class_cond=False, commonGen_train='diffusion_lm/common-gen/commongen_data', config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml', config_name='bert-base-uncased', data_dir='', dataset_config_name='wikitext-2-raw-v1', dataset_name='wikitext', diffusion_steps=2000, dropout=0.1, e2e_train='e2e_data', ema_rate='0.9999', emb_scale_factor=1.0, eval_interval=2000, experiment='random', experiment_mode='lm', fp16_scale_growth=0.001, gradient_clipping=-1.0, image_size=8, in_channel=128, learn_sigma=False, log_interval=50, logits_mode=1, lr=0.0001, lr_anneal_steps=400000, microbatch=-1, modality='bert', model_arch='transformer', model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None', noise_level=0.0, noise_schedule='sqrt', num_channels=128, num_heads=4, num_heads_upsample=-1, num_res_blocks=2, out_channel=128, padding_mode='pad', predict_xstart=True, preprocessing_num_workers=1, rescale_learned_sigmas=True, rescale_timesteps=True, resume_checkpoint='', roc_train='../datasets/ROCstory', save_interval=50000, schedule_sampler='uniform', seed=101, sigma_small=False, timestep_respacing='', training_mode='bert', use_bert_tokenizer='no', use_checkpoint=False, use_fp16=False, use_kl=False, use_scale_shift_norm=True, vocab_size=11043, weight_decay=0.0, wiki_train='diffusion_lm/simple_wiki/data.v1.split/simple.training.txt', yelp_train='diffusion_lm/yelpnlg-resources/yelpnlg-corpus')
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 128, padding_idx=0)
    (position_embeddings): Embedding(512, 128)
    (token_type_embeddings): Embedding(2, 128)
    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-1): 2 x BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=128, out_features=128, bias=True)
            (key): Linear(in_features=128, out_features=128, bias=True)
            (value): Linear(in_features=128, out_features=128, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=128, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=128, out_features=512, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=512, out_features=128, bias=True)
          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
loading dataset from ROCStory
loading from ../datasets/ROCstory
loading form the TRAIN set
[[['brad', 'had', 'trained', 'for', 'the', 'sprint', '.', 'brad', 'was', 'nervous', '.', 'today', 'was', 'the', 'race', '.', 'brad', 'ran', 'the', 'race', '.', 'he', 'got', 'first', 'place', '.']], [['connie', 'loved', 'going', 'to', 'the', 'movies', '.', 'she', 'also', 'enjoyed', 'her', 'high', 'school', 'art', 'class', '.', 'connie', 'wanted', 'a', 'job', 'that', 'combined', 'the', 'things', 'she', 'loved', '.', 'connie', 'major', '##ed', 'in', 'art', 'in', 'college', '.', 'after', 'college', 'she', 'got', 'a', 'job', 'designing', 'posters', 'for', 'movies', '.']]]
here there everywhere
[['brad', 'had', 'trained', 'for', 'the', 'sprint', '.', 'brad', 'was', 'nervous', '.', 'today', 'was', 'the', 'race', '.', 'brad', 'ran', 'the', 'race', '.', 'he', 'got', 'first', 'place', '.']]
Traceback (most recent call last):
  File "scripts/train.py", line 219, in <module>
    main()
  File "scripts/train.py", line 117, in main
    next(data)
  File "/home/ubuntu/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 48, in load_data_text
    training_data, model = get_corpus_rocstory(data_args, model, image_size,
  File "/home/ubuntu/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 618, in get_corpus_rocstory
    result_train_lst = helper_tokenize_encode(sentence_lst, vocab_dict, model, image_size**2, data_args, padding_mode)
  File "/home/ubuntu/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 274, in helper_tokenize_encode
    hidden_state = model(torch.tensor(input_ids))
TypeError: an integer is required (got type list)
[[0, [101, 2018, 102], 1], [0, [101, 3866, 102], 1]]
padding mode is pad