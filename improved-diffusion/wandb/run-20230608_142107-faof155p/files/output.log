creating data loader...
load data **************************************************
mode:  bert
bert loading
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [00:00<00:00, 288kB/s]
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16.9M/16.9M [00:00<00:00, 90.6MB/s]
Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 226k/226k [00:00<00:00, 1.67MB/s]
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-1): 2 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key): Linear(in_features=128, out_features=128, bias=True)
              (value): Linear(in_features=128, out_features=128, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=128, out_features=128, bias=True)
              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=128, out_features=512, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=128, out_features=128, bias=True)
        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=128, out_features=30522, bias=True)
    )
  )
)
hello loading text data.
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-1): 2 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key): Linear(in_features=128, out_features=128, bias=True)
              (value): Linear(in_features=128, out_features=128, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=128, out_features=128, bias=True)
              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=128, out_features=512, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=128, out_features=128, bias=True)
        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=128, out_features=30522, bias=True)
    )
  )
)
Namespace(attention_resolutions='16,8', batch_size=64, cache_mode='no', checkpoint_path='diffusion_models/diff_roc_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e', class_cond=False, commonGen_train='diffusion_lm/common-gen/commongen_data', config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml', config_name='bert-base-uncased', data_dir='', dataset_config_name='wikitext-2-raw-v1', dataset_name='wikitext', diffusion_steps=2000, dropout=0.1, e2e_train='e2e_data', ema_rate='0.9999', emb_scale_factor=1.0, eval_interval=2000, experiment='random', experiment_mode='lm', fp16_scale_growth=0.001, gradient_clipping=-1.0, image_size=8, in_channel=128, learn_sigma=False, log_interval=50, logits_mode=1, lr=0.0001, lr_anneal_steps=400000, microbatch=-1, modality='roc', model_arch='transformer', model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None', noise_level=0.0, noise_schedule='sqrt', num_channels=128, num_heads=4, num_heads_upsample=-1, num_res_blocks=2, out_channel=128, padding_mode='pad', predict_xstart=True, preprocessing_num_workers=1, rescale_learned_sigmas=True, rescale_timesteps=True, resume_checkpoint='', roc_train='../datasets/ROCstory', save_interval=50000, schedule_sampler='uniform', seed=101, sigma_small=False, timestep_respacing='', training_mode='e2e', use_bert_tokenizer='no', use_checkpoint=False, use_fp16=False, use_kl=False, use_scale_shift_norm=True, vocab_size=11043, weight_decay=0.0, wiki_train='diffusion_lm/simple_wiki/data.v1.split/simple.training.txt', yelp_train='diffusion_lm/yelpnlg-resources/yelpnlg-corpus')
**task_mode** bert
loading initialized random embeddings.
IN GET CORPUS ROCSTORY POG POG POG Namespace(attention_resolutions='16,8', batch_size=64, cache_mode='no', checkpoint_path='diffusion_models/diff_roc_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e', class_cond=False, commonGen_train='diffusion_lm/common-gen/commongen_data', config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml', config_name='bert-base-uncased', data_dir='', dataset_config_name='wikitext-2-raw-v1', dataset_name='wikitext', diffusion_steps=2000, dropout=0.1, e2e_train='e2e_data', ema_rate='0.9999', emb_scale_factor=1.0, eval_interval=2000, experiment='random', experiment_mode='lm', fp16_scale_growth=0.001, gradient_clipping=-1.0, image_size=8, in_channel=128, learn_sigma=False, log_interval=50, logits_mode=1, lr=0.0001, lr_anneal_steps=400000, microbatch=-1, modality='roc', model_arch='transformer', model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None', noise_level=0.0, noise_schedule='sqrt', num_channels=128, num_heads=4, num_heads_upsample=-1, num_res_blocks=2, out_channel=128, padding_mode='pad', predict_xstart=True, preprocessing_num_workers=1, rescale_learned_sigmas=True, rescale_timesteps=True, resume_checkpoint='', roc_train='../datasets/ROCstory', save_interval=50000, schedule_sampler='uniform', seed=101, sigma_small=False, timestep_respacing='', training_mode='e2e', use_bert_tokenizer='no', use_checkpoint=False, use_fp16=False, use_kl=False, use_scale_shift_norm=True, vocab_size=11043, weight_decay=0.0, wiki_train='diffusion_lm/simple_wiki/data.v1.split/simple.training.txt', yelp_train='diffusion_lm/yelpnlg-resources/yelpnlg-corpus')
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-1): 2 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key): Linear(in_features=128, out_features=128, bias=True)
              (value): Linear(in_features=128, out_features=128, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=128, out_features=128, bias=True)
              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=128, out_features=512, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=128, out_features=128, bias=True)
        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=128, out_features=30522, bias=True)
    )
  )
)
loading dataset from ROCStory
loading from ../datasets/ROCstory
loading form the TRAIN set
multiprocess.pool.RemoteTraceback:
"""
Traceback (most recent call last):
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 186, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/datasets/fingerprint.py", line 397, in wrapper
    out = func(self, *args, **kwargs)
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 1959, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 1855, in apply_function_on_filtered_inputs
    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 176, in tokenize_function
    examples['text'] = [" ".join(seq) for seq in examples['text']]
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 176, in <listcomp>
    examples['text'] = [" ".join(seq) for seq in examples['text']]
TypeError: sequence item 0: expected str instance, list found
"""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "scripts/train.py", line 219, in <module>
    main()
  File "scripts/train.py", line 117, in main
    next(data)
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 48, in load_data_text
    training_data, model = get_corpus_rocstory(data_args, model, image_size,
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 618, in get_corpus_rocstory
    train_dataset = helper_tokenize_stream(sentence_lst, vocab_dict, model, image_size**2, data_args, padding_mode)
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 182, in helper_tokenize_stream
    tokenized_datasets = raw_datasets.map(
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 1693, in map
    transformed_shards = [r.get() for r in results]
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 1693, in <listcomp>
    transformed_shards = [r.get() for r in results]
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/multiprocess/pool.py", line 771, in get
    raise self._value
TypeError: sequence item 0: expected str instance, list found
[[['brad', 'had', 'trained', 'for', 'the', 'sprint', '.', 'brad', 'was', 'nervous', '.', 'today', 'was', 'the', 'race', '.', 'brad', 'ran', 'the', 'race', '.', 'he', 'got', 'first', 'place', '.']], [['connie', 'loved', 'going', 'to', 'the', 'movies', '.', 'she', 'also', 'enjoyed', 'her', 'high', 'school', 'art', 'class', '.', 'connie', 'wanted', 'a', 'job', 'that', 'combined', 'the', 'things', 'she', 'loved', '.', 'connie', 'major', '##ed', 'in', 'art', 'in', 'college', '.', 'after', 'college', 'she', 'got', 'a', 'job', 'designing', 'posters', 'for', 'movies', '.']]]
save the vocab to diffusion_models/diff_roc_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e/vocab.json
RAM used: 1073.20 MB
Dataset({
    features: ['text'],
    num_rows: 93161
})
RAM used: 1179.20 MB