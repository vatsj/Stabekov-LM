creating data loader...
load data **************************************************
mode:  bert
bert loading
Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-1): 2 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key): Linear(in_features=128, out_features=128, bias=True)
              (value): Linear(in_features=128, out_features=128, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=128, out_features=128, bias=True)
              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=128, out_features=512, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=128, out_features=128, bias=True)
        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=128, out_features=30522, bias=True)
    )
  )
)
hello loading text data.
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-1): 2 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key): Linear(in_features=128, out_features=128, bias=True)
              (value): Linear(in_features=128, out_features=128, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=128, out_features=128, bias=True)
              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=128, out_features=512, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=128, out_features=128, bias=True)
        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=128, out_features=30522, bias=True)
    )
  )
)
Namespace(attention_resolutions='16,8', batch_size=64, cache_mode='no', checkpoint_path='diffusion_models/diff_bert_pad_bert128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e', class_cond=False, commonGen_train='diffusion_lm/common-gen/commongen_data', config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml', config_name='bert-base-uncased', data_dir='', dataset_config_name='wikitext-2-raw-v1', dataset_name='wikitext', diffusion_steps=2000, dropout=0.1, e2e_train='e2e_data', ema_rate='0.9999', emb_scale_factor=1.0, eval_interval=2000, experiment='bert', experiment_mode='lm', fp16_scale_growth=0.001, gradient_clipping=-1.0, image_size=8, in_channel=128, learn_sigma=False, log_interval=50, logits_mode=1, lr=0.0001, lr_anneal_steps=400000, microbatch=-1, modality='bert', model_arch='transformer', model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None', noise_level=0.0, noise_schedule='sqrt', num_channels=128, num_heads=4, num_heads_upsample=-1, num_res_blocks=2, out_channel=128, padding_mode='pad', predict_xstart=True, preprocessing_num_workers=1, rescale_learned_sigmas=True, rescale_timesteps=True, resume_checkpoint='', roc_train='../datasets/ROCstory', save_interval=50000, schedule_sampler='uniform', seed=101, sigma_small=False, timestep_respacing='', training_mode='e2e', use_bert_tokenizer='no', use_checkpoint=False, use_fp16=False, use_kl=False, use_scale_shift_norm=True, vocab_size=11043, weight_decay=0.0, wiki_train='diffusion_lm/simple_wiki/data.v1.split/simple.training.txt', yelp_train='diffusion_lm/yelpnlg-resources/yelpnlg-corpus')
**task_mode** bert
IN GET CORPUS ROCSTORY POG POG POG Namespace(attention_resolutions='16,8', batch_size=64, cache_mode='no', checkpoint_path='diffusion_models/diff_bert_pad_bert128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e', class_cond=False, commonGen_train='diffusion_lm/common-gen/commongen_data', config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml', config_name='bert-base-uncased', data_dir='', dataset_config_name='wikitext-2-raw-v1', dataset_name='wikitext', diffusion_steps=2000, dropout=0.1, e2e_train='e2e_data', ema_rate='0.9999', emb_scale_factor=1.0, eval_interval=2000, experiment='bert', experiment_mode='lm', fp16_scale_growth=0.001, gradient_clipping=-1.0, image_size=8, in_channel=128, learn_sigma=False, log_interval=50, logits_mode=1, lr=0.0001, lr_anneal_steps=400000, microbatch=-1, modality='bert', model_arch='transformer', model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None', noise_level=0.0, noise_schedule='sqrt', num_channels=128, num_heads=4, num_heads_upsample=-1, num_res_blocks=2, out_channel=128, padding_mode='pad', predict_xstart=True, preprocessing_num_workers=1, rescale_learned_sigmas=True, rescale_timesteps=True, resume_checkpoint='', roc_train='../datasets/ROCstory', save_interval=50000, schedule_sampler='uniform', seed=101, sigma_small=False, timestep_respacing='', training_mode='e2e', use_bert_tokenizer='no', use_checkpoint=False, use_fp16=False, use_kl=False, use_scale_shift_norm=True, vocab_size=11043, weight_decay=0.0, wiki_train='diffusion_lm/simple_wiki/data.v1.split/simple.training.txt', yelp_train='diffusion_lm/yelpnlg-resources/yelpnlg-corpus')
BertForMaskedLM(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-1): 2 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=128, out_features=128, bias=True)
              (key): Linear(in_features=128, out_features=128, bias=True)
              (value): Linear(in_features=128, out_features=128, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=128, out_features=128, bias=True)
              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=128, out_features=512, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=512, out_features=128, bias=True)
            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (cls): BertOnlyMLMHead(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=128, out_features=128, bias=True)
        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=128, out_features=30522, bias=True)
    )
  )
)
loading dataset from ROCStory
loading from ../datasets/ROCstory
loading form the TRAIN set
[[['brad', 'had', 'trained', 'for', 'the', 'sprint', '.', 'brad', 'was', 'nervous', '.', 'today', 'was', 'the', 'race', '.', 'brad', 'ran', 'the', 'race', '.', 'he', 'got', 'first', 'place', '.']], [['connie', 'loved', 'going', 'to', 'the', 'movies', '.', 'she', 'also', 'enjoyed', 'her', 'high', 'school', 'art', 'class', '.', 'connie', 'wanted', 'a', 'job', 'that', 'combined', 'the', 'things', 'she', 'loved', '.', 'connie', 'major', '##ed', 'in', 'art', 'in', 'college', '.', 'after', 'college', 'she', 'got', 'a', 'job', 'designing', 'posters', 'for', 'movies', '.']]]
save the vocab to diffusion_models/diff_bert_pad_bert128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e/vocab.json
here there everywhere
[[0, 8226, 2018, 4738, 2005, 1996, 9043, 1012, 8226, 2001, 6091, 1012, 2651, 2001, 1996, 2679, 1012, 8226, 2743, 1996, 2679, 1012, 2002, 2288, 2034, 2173, 1012, 1], [0, 16560, 3866, 2183, 2000, 1996, 5691, 1012, 2016, 2036, 5632, 2014, 2152, 2082, 2396, 2465, 1012, 16560, 2359, 1037, 3105, 2008, 4117, 1996, 2477, 2016, 3866, 1012, 16560, 2350, 1001, 1999, 2396, 1999, 2267, 1012, 2044, 2267, 2016, 2288, 1037, 3105, 12697, 14921, 2005, 5691, 1012, 1]]
padding mode is pad
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Traceback (most recent call last):
  File "scripts/train.py", line 219, in <module>
    main()
  File "scripts/train.py", line 117, in main
    next(data)
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 48, in load_data_text
    training_data, model = get_corpus_rocstory(data_args, model, image_size,
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 628, in get_corpus_rocstory
    result_train_lst = helper_tokenize_encode(sentence_lst, vocab_dict, model, image_size**2, data_args, padding_mode)
  File "/home/jstav/Documents/stanford/Spring 2023/Stabekov-LM/improved-diffusion/improved_diffusion/text_datasets.py", line 289, in helper_tokenize_encode
    hidden_state = model.bert(input_ids2)[0]
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jstav/Documents/stanford/Spring 2023/Diffusion-LM/transformers/src/transformers/models/bert/modeling_bert.py", line 997, in forward
    encoder_outputs = self.encoder(
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jstav/Documents/stanford/Spring 2023/Diffusion-LM/transformers/src/transformers/models/bert/modeling_bert.py", line 586, in forward
    layer_outputs = layer_module(
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jstav/Documents/stanford/Spring 2023/Diffusion-LM/transformers/src/transformers/models/bert/modeling_bert.py", line 473, in forward
    self_attention_outputs = self.attention(
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jstav/Documents/stanford/Spring 2023/Diffusion-LM/transformers/src/transformers/models/bert/modeling_bert.py", line 403, in forward
    self_outputs = self.self(
  File "/home/jstav/miniconda3/envs/golden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jstav/Documents/stanford/Spring 2023/Diffusion-LM/transformers/src/transformers/models/bert/modeling_bert.py", line 341, in forward
    context_layer = torch.matmul(attention_probs, value_layer)
KeyboardInterrupt
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0