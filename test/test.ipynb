{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "config_encoder = BertConfig()\n",
    "config_decoder = BertConfig()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# defines start token id\n",
    "config_decoder.bos_token_id = tokenizer.cls_token_id\n",
    "\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "model = EncoderDecoderModel(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
    "\n",
    "# defines start token\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". and and and and and and and and and and and and and and and and and and\n"
     ]
    }
   ],
   "source": [
    "# let's perform inference on a long piece of text\n",
    "ARTICLE_TO_SUMMARIZE = (\"\"\"\n",
    "the quick brown fox jumped over the lazy dog\n",
    "\"\"\")\n",
    "input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# autoregressively generate summary (uses greedy decoding by default)\n",
    "generated_ids = model.generate(input_ids)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  hello my name is jacob\n",
      "tokenized:  tensor([[ 101, 7592, 2026, 2171, 2003, 6213,  102]])\n",
      "encoded:  BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1077,  0.1250, -0.2416,  ..., -2.6587, -0.2152,  2.9175],\n",
      "         [ 0.0961, -1.1708, -0.2107,  ..., -1.6276, -0.9084, -0.0701],\n",
      "         [ 0.0304, -1.4213, -0.7568,  ..., -2.0410,  0.8679,  0.4443],\n",
      "         ...,\n",
      "         [-0.4005, -0.7860,  0.5107,  ..., -1.1150,  0.7879,  1.3168],\n",
      "         [-0.3581,  0.8284, -0.3324,  ..., -2.1218, -0.2689,  1.4055],\n",
      "         [ 0.4212, -0.3092, -0.1997,  ..., -1.6308, -0.4390,  1.4286]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.5772e-01,  2.4764e-01,  2.7204e-01, -7.5996e-01,  7.3690e-02,\n",
      "         -4.1270e-01, -2.4195e-01, -2.1879e-02,  1.4277e-02,  2.6252e-02,\n",
      "          1.2941e-01, -1.2035e-01, -2.5512e-01,  6.2538e-01,  3.3608e-01,\n",
      "          3.9040e-01, -3.0259e-01, -5.3704e-01,  4.9539e-01, -7.1988e-01,\n",
      "          2.5492e-01,  1.4894e-01, -1.3046e-01, -5.6949e-01, -3.9556e-01,\n",
      "         -6.3785e-01, -3.5459e-01, -9.6662e-02, -3.3556e-02, -1.3928e-01,\n",
      "         -5.8910e-01,  2.6635e-01,  6.9147e-03,  5.4314e-01,  7.0051e-01,\n",
      "         -3.1061e-01, -1.0951e-01,  8.2795e-02, -1.0933e-01,  2.9518e-01,\n",
      "          3.0946e-01,  4.4195e-01, -6.2618e-01,  1.5281e-01,  1.0449e-01,\n",
      "          2.6485e-01,  6.0544e-02, -1.1195e-01, -3.4109e-01, -5.4643e-01,\n",
      "          9.9472e-02, -5.4514e-01, -3.2553e-01, -4.8554e-01,  3.5862e-01,\n",
      "         -8.5019e-01, -5.4185e-01,  5.1888e-01,  8.1228e-02,  8.0881e-02,\n",
      "          5.4639e-01, -5.6108e-01, -7.9300e-01, -7.2386e-03, -5.2085e-01,\n",
      "         -7.6253e-01, -2.4949e-01,  6.2687e-01,  3.9682e-01, -2.7008e-01,\n",
      "          8.7142e-03,  5.9729e-01,  2.3847e-01,  3.0245e-01, -1.7833e-01,\n",
      "          6.2235e-01,  2.4508e-02, -1.3116e-01,  7.3605e-02,  1.7688e-01,\n",
      "          6.1397e-01,  1.8055e-01,  3.5002e-01,  1.6026e-01,  9.2788e-02,\n",
      "         -4.5375e-01, -2.6481e-01,  6.0907e-01, -3.8695e-01, -8.7393e-01,\n",
      "         -4.5025e-01,  5.1671e-01,  2.6236e-01,  5.9610e-01,  4.2255e-01,\n",
      "          5.8599e-02, -6.4220e-01, -5.7964e-02, -3.6462e-02, -5.6976e-01,\n",
      "          7.1889e-03,  1.7131e-01, -3.3078e-03,  6.6315e-01,  4.5757e-01,\n",
      "          1.2149e-03,  5.6981e-01,  1.1811e-01,  7.6565e-01,  1.9498e-01,\n",
      "          4.7493e-01,  1.4921e-01,  6.1750e-01,  4.5681e-01, -2.5305e-01,\n",
      "         -1.2158e-01,  2.1594e-01,  4.5474e-01, -2.8812e-01,  6.3059e-02,\n",
      "         -5.1552e-01, -7.3642e-01,  4.2335e-01, -3.1923e-01,  7.1698e-02,\n",
      "          6.9616e-01,  2.5179e-01,  8.0818e-01,  2.5181e-01,  6.3495e-01,\n",
      "          1.3101e-01, -1.5207e-01,  4.9175e-01,  4.4959e-01, -8.3660e-01,\n",
      "          8.3705e-02, -1.9758e-01,  5.5806e-01,  1.5066e-01,  1.4979e-01,\n",
      "         -6.7946e-01, -3.2276e-01, -1.5472e-01,  7.7405e-02, -3.3034e-02,\n",
      "          5.4539e-01, -1.7046e-01, -3.0239e-01,  4.9936e-01, -6.3949e-01,\n",
      "         -3.4153e-01, -2.5211e-01, -7.4143e-01, -2.9112e-01,  4.1864e-01,\n",
      "         -2.6582e-01, -2.4500e-01,  1.6057e-01,  2.1585e-02,  5.2016e-01,\n",
      "          6.1924e-01, -1.6060e-01,  3.5949e-01, -6.5546e-01,  7.9696e-01,\n",
      "         -3.5058e-01,  2.7219e-01, -5.1021e-01,  2.9525e-01,  8.1449e-01,\n",
      "         -4.4396e-01,  4.2979e-01, -3.7893e-01, -4.4712e-01,  2.4213e-01,\n",
      "         -1.0061e-01, -5.7636e-01,  7.4168e-01, -5.1337e-01,  1.0179e-01,\n",
      "         -2.4916e-01, -8.2182e-01,  4.2573e-01,  3.8028e-02,  6.9459e-01,\n",
      "         -1.4370e-01,  1.0171e-01, -9.4078e-01, -2.3922e-01, -4.3876e-02,\n",
      "          1.0843e-01, -3.6672e-01,  1.6513e-01, -4.7797e-01,  1.8652e-02,\n",
      "          2.2535e-01,  7.0055e-01, -5.0592e-01,  6.5571e-01, -3.3795e-01,\n",
      "          4.7739e-01, -2.4723e-01,  7.4653e-01,  1.9926e-01,  4.2026e-01,\n",
      "         -4.4175e-01,  3.4552e-02,  9.6293e-02,  3.8771e-01, -6.8437e-03,\n",
      "          1.4681e-01,  3.6271e-01, -1.1512e-01, -4.0020e-01,  1.5750e-01,\n",
      "          2.0643e-01, -1.2398e-01,  4.2000e-01, -2.6047e-01, -1.7580e-01,\n",
      "          6.1060e-01,  2.0205e-01, -4.1388e-01,  5.4842e-01, -4.9195e-01,\n",
      "          7.1478e-01, -1.0981e-01, -1.8406e-02, -7.1087e-01, -2.1755e-01,\n",
      "          2.4483e-01,  1.5711e-01, -6.8901e-02,  2.8642e-01, -2.8569e-01,\n",
      "          3.4708e-01,  8.2002e-01, -7.5202e-03,  3.6385e-01,  6.8508e-01,\n",
      "         -1.3867e-02,  6.8841e-01, -3.9455e-01, -1.0883e-01,  4.9946e-01,\n",
      "          8.6103e-01,  2.9878e-02,  5.3142e-01,  7.2282e-01,  3.9929e-01,\n",
      "         -3.7357e-01,  1.4385e-01,  3.6192e-01, -6.1036e-01, -2.7699e-02,\n",
      "          6.4907e-01,  5.1167e-01,  2.7307e-02, -8.5529e-01, -5.9916e-01,\n",
      "         -6.4444e-02,  5.3314e-01, -4.3521e-01,  1.9288e-01,  2.2951e-01,\n",
      "          3.1096e-01, -2.6386e-01,  3.0290e-01,  7.7663e-02,  3.2374e-01,\n",
      "         -9.0539e-01,  2.4110e-01,  1.0125e-01,  5.2737e-01, -5.9749e-01,\n",
      "          3.1885e-02, -7.1427e-01, -3.9659e-01, -3.4529e-01,  3.8917e-01,\n",
      "          4.4009e-01,  8.9302e-01, -7.0945e-01, -3.4988e-01, -7.1847e-02,\n",
      "          8.2348e-01,  5.1980e-01, -5.1574e-01,  3.2522e-01,  5.2053e-01,\n",
      "         -3.3184e-02,  3.6116e-01, -6.8660e-02, -7.9443e-01, -9.0599e-01,\n",
      "         -4.3122e-01, -1.2579e-01,  2.6709e-01, -2.5995e-01,  6.5751e-01,\n",
      "         -4.2157e-01,  7.2393e-01, -2.9958e-01,  4.5009e-01, -9.3605e-02,\n",
      "          3.4410e-01,  6.1852e-01,  4.1823e-01, -3.5811e-01,  5.7862e-01,\n",
      "         -1.0366e-01, -4.7497e-01, -9.7717e-02,  6.3549e-01,  3.5673e-01,\n",
      "         -1.8036e-01, -8.3780e-01,  7.1453e-01, -3.4907e-01, -8.1958e-03,\n",
      "          7.8637e-02,  1.4170e-02, -6.8226e-01, -3.9058e-01,  3.4714e-01,\n",
      "          3.8624e-01, -3.0975e-01,  6.6884e-01, -2.5397e-01,  4.4518e-01,\n",
      "          2.2994e-01, -4.9608e-01, -1.3738e-01,  4.9246e-01, -7.0580e-01,\n",
      "         -2.1729e-01, -4.2572e-01, -6.7213e-02,  4.3945e-03,  6.4539e-01,\n",
      "         -8.2967e-01,  4.0457e-01,  6.0310e-01,  4.5935e-01,  2.7721e-01,\n",
      "         -2.1839e-01,  4.4481e-01,  1.8832e-01, -4.2204e-02,  2.2162e-01,\n",
      "         -2.5871e-01, -8.2052e-01, -3.1470e-02, -8.3489e-02,  5.8522e-01,\n",
      "          8.4383e-01,  2.9210e-01,  7.7245e-01,  7.2431e-01, -5.8838e-01,\n",
      "         -5.4070e-01, -6.0654e-02, -5.7384e-01, -4.3537e-01,  4.7612e-01,\n",
      "         -5.9768e-01, -4.6686e-01,  4.8106e-01,  4.6950e-03,  4.9925e-01,\n",
      "          3.5253e-01,  5.6045e-01, -4.2470e-01, -6.6110e-01, -5.4395e-01,\n",
      "         -2.1806e-01,  6.8826e-01, -4.5315e-01,  4.9661e-01, -5.5372e-02,\n",
      "         -6.5958e-01, -5.2330e-01,  2.3962e-02,  2.2041e-01,  9.2575e-02,\n",
      "          4.5042e-01, -3.0678e-01,  5.8179e-01,  1.7239e-01, -1.3958e-01,\n",
      "         -1.4279e-01,  4.5702e-01, -3.7121e-01, -3.9000e-01, -5.4976e-01,\n",
      "          6.1330e-01,  5.3506e-01, -8.8542e-02, -9.2873e-01, -1.4893e-01,\n",
      "         -3.9426e-01,  7.6766e-01,  1.9723e-01, -7.5301e-01, -2.5986e-01,\n",
      "          5.6704e-01, -4.0846e-01, -6.0926e-01,  3.4555e-01, -3.9865e-01,\n",
      "         -5.2239e-02,  5.5549e-01,  2.8764e-01, -6.5904e-01, -1.6811e-01,\n",
      "          8.4103e-02, -3.4894e-01,  6.6035e-01, -6.6798e-01, -1.4852e-01,\n",
      "          4.2629e-01,  1.2953e-01,  4.5635e-02,  4.6894e-04,  8.1948e-02,\n",
      "          5.7742e-01, -1.5879e-03, -2.2195e-02, -7.1635e-01, -2.6157e-01,\n",
      "          1.6859e-01,  6.8111e-02, -2.2948e-01,  2.6185e-01,  1.6101e-01,\n",
      "         -5.9867e-01,  2.8280e-01, -4.5578e-01,  1.1794e-01, -6.0840e-01,\n",
      "          2.4292e-01, -6.8234e-01, -6.5099e-01,  4.4253e-01,  1.4991e-02,\n",
      "          4.7797e-03,  3.6400e-01,  7.0646e-01, -3.6100e-02, -6.3056e-01,\n",
      "         -2.5701e-01, -4.8345e-01,  6.2194e-01, -3.6289e-02, -3.8477e-01,\n",
      "          7.1516e-01,  6.7760e-01,  3.9127e-01,  4.3928e-01, -7.0959e-02,\n",
      "         -1.3277e-01, -3.1634e-01, -4.3199e-01, -7.6637e-01,  6.0961e-01,\n",
      "         -2.7728e-02,  1.5973e-01,  2.2221e-01,  2.5611e-01, -7.2616e-01,\n",
      "         -1.5567e-01, -3.0128e-02,  2.3229e-01,  1.9680e-01, -7.1323e-01,\n",
      "          4.7634e-01,  6.6364e-01,  5.0249e-02, -8.2336e-01, -7.1796e-01,\n",
      "          8.9718e-01, -1.9997e-01,  3.3485e-01,  4.2121e-01,  3.6717e-01,\n",
      "          3.0528e-01, -7.5316e-01, -5.9692e-02, -5.5514e-01,  3.5077e-01,\n",
      "          3.2364e-01, -8.6441e-02,  1.1357e-01,  2.4923e-01, -3.8361e-02,\n",
      "         -5.4208e-01,  1.0800e-01, -1.3491e-01,  3.7600e-02, -4.8252e-01,\n",
      "          2.1747e-01, -8.2753e-01, -7.3597e-01, -5.6379e-01, -1.7293e-01,\n",
      "          2.3600e-01,  5.6532e-01, -3.4659e-01,  5.3943e-01,  8.1863e-02,\n",
      "         -7.2178e-01,  9.1797e-01, -1.8837e-01,  4.0978e-01, -3.1977e-01,\n",
      "          7.2161e-01,  6.1831e-01,  3.8423e-01,  2.6840e-01,  9.5469e-02,\n",
      "         -6.8353e-01, -2.6473e-01,  3.8740e-01, -7.4652e-01, -4.6792e-01,\n",
      "         -7.7784e-02,  2.1814e-01, -8.7247e-01, -1.0016e-01,  4.8842e-01,\n",
      "          5.0417e-01,  1.4869e-01,  1.1610e-01, -6.8931e-01, -5.6341e-01,\n",
      "          4.9348e-02, -2.9960e-01,  4.5257e-01,  5.1845e-01,  4.2440e-01,\n",
      "         -5.8787e-01,  7.8261e-01, -3.5021e-01, -2.5950e-01,  2.1951e-01,\n",
      "         -2.7310e-01,  5.2911e-01, -7.4672e-01, -1.9819e-01,  4.7733e-01,\n",
      "         -3.8093e-01,  6.1916e-01,  3.9824e-01, -3.1926e-02,  2.6652e-01,\n",
      "         -5.1343e-02, -6.2881e-01,  4.1301e-02,  7.1615e-01,  2.2367e-01,\n",
      "         -1.5549e-01,  6.2703e-01,  3.6304e-01,  9.9053e-03,  4.0225e-01,\n",
      "          3.9883e-03,  9.9232e-02,  1.2884e-01,  3.3789e-01, -5.4576e-01,\n",
      "         -3.3713e-01, -3.9590e-01, -7.8946e-01,  4.1317e-01,  4.3547e-01,\n",
      "         -1.6602e-01,  2.8524e-01, -6.6412e-01, -7.4572e-01, -4.9141e-02,\n",
      "          3.9600e-01,  4.2593e-01,  7.3635e-02, -3.6684e-01, -3.0195e-01,\n",
      "         -1.2445e-01,  6.5243e-01, -1.9466e-01, -6.5511e-02, -9.3734e-02,\n",
      "         -4.2027e-01, -3.5051e-01, -3.8562e-01,  4.2428e-01, -6.0742e-01,\n",
      "          9.3471e-01,  2.3739e-01,  9.7416e-03, -3.6347e-01, -3.1611e-01,\n",
      "          1.0135e-01,  4.9864e-01, -4.7975e-01,  3.6649e-01,  8.9144e-01,\n",
      "          6.6450e-02, -2.1110e-02,  2.7477e-01, -3.5662e-01, -5.4692e-01,\n",
      "          3.8190e-01, -1.3928e-01, -2.8183e-01,  8.5659e-02,  7.2570e-01,\n",
      "          3.5712e-01,  4.7421e-01,  2.3620e-01,  4.7364e-01,  7.9750e-01,\n",
      "          1.7414e-01,  2.9977e-01,  4.6138e-01, -2.3985e-02, -5.6802e-01,\n",
      "          1.0254e-01,  6.4425e-01,  3.2767e-01,  2.8313e-01, -1.5520e-01,\n",
      "          1.7278e-01,  7.8021e-01, -5.7400e-01,  7.5368e-01, -5.9089e-01,\n",
      "          2.3574e-01,  7.2393e-01,  3.0215e-01,  4.9470e-01,  5.4493e-01,\n",
      "         -6.5405e-02,  6.5693e-01,  4.5675e-01,  5.1513e-01, -2.7665e-01,\n",
      "         -4.9366e-01,  1.1853e-01, -2.9370e-01,  6.9917e-01,  6.1481e-01,\n",
      "         -3.7652e-01,  6.5845e-01,  9.2613e-02,  3.6780e-01, -4.6701e-01,\n",
      "         -5.5717e-02,  6.2947e-01, -3.9959e-02,  5.5153e-02, -4.5164e-01,\n",
      "          6.7252e-01,  3.8096e-01,  4.2326e-01,  8.0774e-01, -6.7075e-01,\n",
      "          8.0919e-01, -3.6170e-01, -7.0487e-01,  4.7993e-01, -7.9177e-01,\n",
      "          6.1898e-02, -1.2561e-01,  2.9086e-01, -1.5261e-02, -4.9377e-01,\n",
      "         -9.1557e-01,  6.1011e-01, -8.6623e-02, -3.3491e-01, -4.7253e-01,\n",
      "         -3.4816e-01, -3.8877e-01,  2.6377e-01, -2.8291e-01, -6.0834e-01,\n",
      "         -8.5097e-02,  7.7380e-01, -7.3847e-01, -1.9861e-01,  1.4484e-01,\n",
      "         -2.9816e-01, -6.3073e-01,  3.3232e-01, -6.7274e-01,  4.3289e-01,\n",
      "         -1.5547e-01,  1.9623e-01, -2.9765e-01, -6.2905e-01,  1.2523e-02,\n",
      "          1.2579e-01,  7.6194e-02, -2.8366e-01, -7.6129e-01,  8.8770e-02,\n",
      "         -1.2367e-01,  3.3284e-01,  3.5374e-02,  8.2334e-02,  3.8689e-01,\n",
      "          6.2992e-01, -4.3641e-01,  4.6912e-01,  2.2808e-01, -2.1630e-01,\n",
      "         -2.5378e-01, -1.2391e-01, -5.5294e-01,  6.2186e-01,  6.0372e-01,\n",
      "          7.8152e-01,  4.4541e-01, -1.6978e-01,  3.5983e-01,  1.3099e-01,\n",
      "          9.5022e-02,  8.8914e-02, -7.0385e-01,  4.1248e-01,  9.1765e-02,\n",
      "         -8.1081e-01,  1.7269e-02, -4.3073e-01,  4.7698e-01,  3.3626e-01,\n",
      "         -8.9895e-01, -1.4037e-01, -7.7991e-01, -7.0987e-02,  8.5551e-02,\n",
      "          4.4560e-02, -6.8076e-01, -4.1190e-01, -4.7957e-01, -3.8675e-01,\n",
      "          5.3697e-01,  8.0889e-02,  3.1147e-01,  2.9800e-01,  4.9227e-02,\n",
      "          1.3978e-01,  1.9672e-01,  4.2693e-01,  1.0234e-01,  1.7901e-01,\n",
      "         -1.8590e-01,  8.3619e-01,  1.8239e-01, -1.4054e-01,  7.9234e-02,\n",
      "          2.6429e-01,  5.8341e-03,  6.1292e-01, -8.4429e-01, -3.5029e-02,\n",
      "          4.9257e-01,  4.5262e-01,  5.6151e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m encoded \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder(tokenized)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mencoded: \u001b[39m\u001b[39m\"\u001b[39m, encoded)\n\u001b[0;32m----> 7\u001b[0m decoded \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecoder(encoded)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdecoded: \u001b[39m\u001b[39m\"\u001b[39m, decoded)\n\u001b[1;32m      9\u001b[0m sentence_prime \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(decoded\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1234\u001b[0m, in \u001b[0;36mBertLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1235\u001b[0m     input_ids,\n\u001b[1;32m   1236\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1237\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1238\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1239\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1240\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1241\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1242\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1243\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1244\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1245\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1246\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1247\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1248\u001b[0m )\n\u001b[1;32m   1250\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1251\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:967\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    966\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    968\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "sentence = \"hello my name is jacob\"\n",
    "print(\"sentence: \", sentence)\n",
    "tokenized = tokenizer.encode(sentence, return_tensors='pt')\n",
    "print(\"tokenized: \", tokenized)\n",
    "encoded = model.encoder(tokenized)\n",
    "print(\"encoded: \", encoded)\n",
    "decoded = model.decoder(encoded)\n",
    "print(\"decoded: \", decoded)\n",
    "sentence_prime = tokenizer.decode(decoded.argmax(axis=-1))\n",
    "print(\"sentence prime: \", sentence_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
